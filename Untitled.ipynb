{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "## Single Layer Feedforward\n",
    "Input->layer->output\n",
    "For n inputs, with **weight** $\\omega$ and **bias** $b$\n",
    "$$\\displaystyle y = \\sum_{i=0}^{n} x_i \\omega_i + b $$\n",
    "or for weight vector $\\omega$ and input vector $x$\n",
    "$$y=\\mathbf{\\omega} \\cdot \\mathbf{x}+b$$\n",
    "Passing through an **sigmoid** function to **feedforward** an output\n",
    "(Activation Functions also contains **Sigmoid** **TanH** **ReLU**)\n",
    "\n",
    "$${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}.}$$\n",
    "for first derivative as\n",
    "\n",
    "$$S^` = S(1-S)$$\n",
    "\n",
    "$$ y_{1} = S(\\omega \\cdot x+b)=S(y)$$\n",
    "## Multi Layer Feedford\n",
    "Input->layer1->layer2->output\n",
    "\n",
    "repeat feedforwarding\n",
    "$$ y_{1} = S(\\omega \\cdot x+b)$$\n",
    "\n",
    "$$ y_{2} = S(\\omega \\cdot y_1 +b)$$\n",
    "\n",
    "$$ \\dots$$\n",
    "\n",
    "$$y_{n} = S(\\omega \\cdot y_{n-1}+b)$$\n",
    "\n",
    "## Training\n",
    "\n",
    "$y_{true}$ is the reference correct value, $y_{pred}$ is the value of output from feedfording.\n",
    "\n",
    "**Mean Squared Error (MSE) loss** is\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_{true}-y_{pred})^2 $$\n",
    "\n",
    "Training is going to **minimize** the MSE loss by iterational changing the weight $\\omega$ and bias $b$.\n",
    "\n",
    "For weight $\\omega_1$, the **backpropagation**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\omega_1} = \\frac{\\partial L}{\\partial y_{pred}}  \\frac{\\partial y_{pred}}{\\partial y_{n-1}} \\frac{\\partial y_{n-1}}{\\partial y_{n-2}} \\ldots \\frac{\\partial y_{2}}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial \\omega_1}$$\n",
    "\n",
    "remark that each partial derivative contains a part formed $S(1-S)$ as showed before.\n",
    "\n",
    "Then a **stochastic gradient descent(SGD)** is used for minimize weight  with **learning rate $\\eta$** as\n",
    "$$\\omega_{1}^{(1)}= \\omega_1 - \\eta \\frac{\\partial L}{\\partial \\omega_1}$$\n",
    "\n",
    "After training  **epochs** times, getting the optimize weight and bias, then use this system to predict new inputs.\n",
    "\n",
    "# Convolutional NN\n",
    "\n",
    "Basic hidden layer have 1024 nodes, big amount of input features may cause large number of weights. Also to look at every frame in the first layer is not necessary.\n",
    "\n",
    "## Conv layers\n",
    "\n",
    " **Convolving** the smaller dimision **filter** with larger dimision input signal. (Sometimes instead of convolution, use cross-correlation), each result replace the original bunch of slices of signal.\n",
    "\n",
    "## Pooling\n",
    "\n",
    "Reducing the number of redundant similar features by using max/min/average of a group of neighbers instead of using all of them.\n",
    "For example, for size 4 neighber{0,10,20,30}(in picture will be 4*4), max pooling will do maxpooling{0,10,20,30} => {30}.\n",
    "\n",
    "## Softmax\n",
    "\n",
    "Finally layer, which comming out of n nodes, those digits represented by the node with the highest probability as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
